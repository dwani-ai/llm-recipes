LLM Recipes

Usage of LLM for Everyday use

- v1
    - Prompt Optimization - DSPy + Mixtral + Ollama/Mistral API
        - Docs at [docs/dspy.md](https://github.com/slabstech/llm-recipes/blob/main/docs/dspy.md)
        - Code examples at [src/dspy](https://github.com/slabstech/llm-recipes/tree/main/src/dspy)
    - Agents : autogen + vllm + gemma
        - [VLLM setup](https://github.com/slabstech/llm-recipes/blob/main/docs/vllm.md) 
    - Agents : autogen + ollama + gemma
        - Setup + Documentation at [docs/2024/agent-code.md](https://github.com/slabstech/llm-recipes/blob/main/docs/2024/agent-code.md) 
        - Code examples at [src/autogen](https://github.com/slabstech/llm-recipes/tree/main/src/autogen)
        - Output from examples at [docs/2024/agent-example-output.md](https://github.com/slabstech/llm-recipes/blob/main/docs/2024/agent-example-output.md)
    - llama.cpp + Raspi 4
        - [Docs](https://github.com/slabstech/llm-recipes/blob/main/docs/llama-cpp.md) for setup of Raspi 4 inference. 
- v0
    - ChatUI  : ollama + open-webui + mistral-7B + docker
        - Setup + Documentation at [docs/ollama-open-webui.md](https://github.com/slabstech/llm-recipes/blob/main/docs/ollama-open-webui.md)
    - Code CoPilot : vscode + continue + ollama + mistral-7B
        - Setup document at [docs/code-pair.md](https://github.com/slabstech/llm-recipes/blob/main/docs/code-pair.md)

Extra 
 - [Clean install](https://github.com/slabstech/llm-recipes/blob/main/docs/clean-ubuntu-setup.md) of ubuntu + docker + nvidia requirements
 