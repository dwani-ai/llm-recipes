Codestral Mamba

- Run with continue

- Setup
  - mistral-inference
    - python3.10 -m venv venv
    - source venv/bin/activate
    - pip install mistral-inference
    - pip install packaging mamba-ssm causal-conv1d transformers
    - 
    - https://github.com/mistralai/mistral-inference


  - https://mistral.ai/news/codestral-mamba/
  - https://docs.vllm.ai/en/latest/models/supported_models.html
  - https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1
  - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba
  - https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/

  - Paper
    - Mamba 1 - https://arxiv.org/abs/2312.00752
    - Mamba 2 - https://arxiv.org/pdf/2405.21060




- Milestones
  - Learn how to utilise CUDA kernels
  - Implement mamba2 from scratch to run codestral locally

  - Make it compatible with llama.cpp and later to ollama


- Reference
  - https://mistral.ai/news/codestral-mamba/
  - mamba - research - https://arxiv.org/abs/2312.00752 
  - https://huggingface.co/mistralai/mamba-codestral-7B-v0.1
  - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba
  - https://github.com/state-spaces/mamba
  - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba


