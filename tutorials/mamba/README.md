Codestral Mamba

- Milestones
  - Learn how to utilise CUDA kernels
  - Implement mamba2 from scratch to run codestral locally

  - Make it compatible with llama.cpp and later to ollama


- Reference
  - https://mistral.ai/news/codestral-mamba/
  - mamba - research - https://arxiv.org/abs/2312.00752 
  - https://huggingface.co/mistralai/mamba-codestral-7B-v0.1
  - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba
  - https://github.com/state-spaces/mamba
  - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba


