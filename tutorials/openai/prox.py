from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel, ValidationError
from typing import List, Dict, Optional
import requests
import time
import logging
from dotenv import load_dotenv
import os
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(title="vLLM Proxy Server with OpenAI-Compatible API")

# vLLM server configuration
VLLM_API_BASE = os.getenv("VLLM_API_BASE", "http://localhost:8000/v1")
VLLM_API_KEY = os.getenv("VLLM_API_KEY", "")

# HTTP headers for vLLM requests
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {VLLM_API_KEY}" if VLLM_API_KEY else None
}
headers = {k: v for k, v in headers.items() if v is not None}

# In-memory storage for rate limiting
rate_limit_store = defaultdict(list)

# Pydantic models for OpenAI-compatible API
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    top_p: Optional[float] = 1.0
    stream: Optional[bool] = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None

class Usage(BaseModel):
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Optional[Usage] = None

# Custom features
class ProxyFeatures:
    @staticmethod
    def log_request(request: Dict, client_ip: str) -> None:
        """Log incoming request details."""
        logger.info(
            f"Request from {client_ip}: model={request.get('model')}, "
            f"messages={len(request.get('messages', []))} messages"
        )

    @staticmethod
    def log_response(response: Dict, processing_time: Optional[float] = None) -> None:
        """Log response details."""
        logger.info(
            f"Response: id={response.get('id')}, choices={len(response.get('choices', []))}, "
            f"processing_time={processing_time:.2f}s" if processing_time else "processing_time=unknown"
        )

    @staticmethod
    def rate_limit(client_ip: str, max_requests: int = 10, window_seconds: int = 60) -> bool:
        """In-memory rate limiting per client IP."""
        current_time = time.time()
        rate_limit_store[client_ip] = [
            t for t in rate_limit_store[client_ip] if current_time - t < window_seconds
        ]
        rate_limit_store[client_ip].append(current_time)
        if len(rate_limit_store[client_ip]) > max_requests:
            logger.warning(f"Rate limit exceeded for {client_ip}")
            return False
        return True

    @staticmethod
    def modify_response(response: Dict) -> Dict:
        """Modify response (e.g., add disclaimer)."""
        for choice in response.get('choices', []):
            choice['message']['content'] = (
                f"{choice['message']['content']}\n\n*Disclaimer: Generated by AI, verify before use.*"
            )
        return response

    @staticmethod
    def estimate_usage(request: Dict, response: Dict) -> Dict:
        """Estimate token usage if not provided by vLLM."""
        if response.get('usage') is None or not all(
            key in response['usage'] for key in ['prompt_tokens', 'completion_tokens', 'total_tokens']
        ):
            prompt_text = ' '.join(msg.get('content', '') for msg in request.get('messages', []))
            response_text = response.get('choices', [{}])[0].get('message', {}).get('content', '')
            # Rough token estimation (1 token ~ 4 characters, minimum 1 token)
            prompt_tokens = max(len(prompt_text) // 4, 1)
            completion_tokens = max(len(response_text) // 4, 1)
            response['usage'] = {
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': prompt_tokens + completion_tokens
            }
        return response

# Chat completions endpoint
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest, fastapi_request: Request):
    """Proxy chat completion requests to vLLM with custom features."""
    start_time = time.time()
    client_ip = fastapi_request.client.host

    # Log request
    ProxyFeatures.log_request(request.dict(), client_ip)

    # Rate limiting
    if not ProxyFeatures.rate_limit(client_ip):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")

    # Forward request to vLLM
    if request.stream:
        raise HTTPException(status_code=501, detail="Streaming not supported yet")

    try:
        response = requests.post(
            f"{VLLM_API_BASE}/chat/completions",
            json=request.dict(),
            headers=headers,
            timeout=30
        )
        response.raise_for_status()
        vllm_response = response.json()

        # Log raw response for debugging
        logger.debug(f"vLLM raw response: {vllm_response}")

        # Estimate usage if missing
        vllm_response = ProxyFeatures.estimate_usage(request.dict(), vllm_response)

        # Modify response
        modified_response = ProxyFeatures.modify_response(vllm_response)

        # Log response
        processing_time = time.time() - start_time
        ProxyFeatures.log_response(modified_response, processing_time)

        # Validate and return response
        return ChatCompletionResponse(**modified_response)

    except requests.RequestException as e:
        logger.error(f"vLLM server error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"vLLM error: {str(e)}")
    except ValidationError as e:
        logger.error(f"Response validation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Response validation error: {str(e)}")

# Health check endpoint
@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)