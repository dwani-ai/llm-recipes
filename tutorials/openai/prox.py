from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel, ValidationError
from typing import List, Dict, Optional
import requests
import time
import logging
from dotenv import load_dotenv
import os
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(title="vLLM Proxy Server with OpenAI-Compatible API")

# vLLM server configuration
VLLM_API_BASE = os.getenv("VLLM_API_BASE", "http://localhost:8000/v1")
VLLM_API_KEY = os.getenv("VLLM_API_KEY", "")

# HTTP headers for vLLM requests
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {VLLM_API_KEY}" if VLLM_API_KEY else None
}
headers = {k: v for k, v in headers.items() if v is not None}

# In-memory storage for rate limiting
rate_limit_store = defaultdict(list)

# Pydantic models for OpenAI-compatible API
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    top_p: Optional[float] = 1.0
    stream: Optional[bool] = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Optional[Dict[str, int]] = None

# Custom features
class ProxyFeatures:
    @staticmethod
    def log_request(request: Dict, client_ip: str) -> None:
        """Log incoming request details."""
        logger.info(
            f"Request from {client_ip}: model={request.get('model')}, "
            f"messages={len(request.get('messages', []))} messages"
        )

    @staticmethod
    def log_response(response: Dict, processing_time: float) -> None:
        """Log response details."""
        logger.info(
            f"Response: id={response.get('id')}, choices={len(response.get('choices', []))}, "
            f"processing_time={processing_time:.2f}s"
        )

    @staticmethod
    def rate_limit(client_ip: str, max_requests: int = 10, window_seconds: int = 60) -> bool:
        """In-memory rate limiting per client IP."""
        current_time = time.time()
        rate_limit_store[client_ip] = [
            t for t in rate_limit_store[client_ip] if current_time - t < window_seconds
        ]
        rate_limit_store[client_ip].append(current_time)
        if len(rate_limit_store[client_ip]) > max_requests:
            logger.warning(f"Rate limit exceeded for {client_ip}")
            return False
        return True

    @staticmethod
    def modify_response(response: Dict) -> Dict:
        """Modify response (e.g., add disclaimer)."""
        for choice in response.get('choices', []):
            choice['message']['content'] = (
                f"{choice['message']['content']}\n\n*Disclaimer: Generated by AI, verify before use.*"
            )
        return response

# Chat completions endpoint
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest, fastapi_request: Request):
    """Proxy chat completion requests to vLLM with custom features."""
    start_time = time.time()
    client_ip = fastapi_request.client.host

    # Log request
    ProxyFeatures.log_request(request.dict(), client_ip)

    # Rate limiting
    if not ProxyFeatures.rate_limit(client_ip):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")

    # Forward request to vLLM
    if request.stream:
        raise HTTPException(status_code=501, detail="Streaming not supported yet")

    try:
        response = requests.post(
            f"{VLLM_API_BASE}/chat/completions",
            json=request.dict(),
            headers=headers,
            timeout=30
        )
        response.raise_for_status()
        vllm_response = response.json()

        # Modify response
        modified_response = ProxyFeatures.modify_response(vllm_response)

        # Log response
        processing_time = time.time() - start_time
        ProxyFeatures.log_response(modified_response, processing_time)

        # Validate and return response
        return ChatCompletionResponse(**modified_response)

    except requests.RequestException as e:
        raise HTTPException(status_code=500, detail=f"vLLM error: {str(e)}")
    except ValidationError as e:
        raise HTTPException(status_code=500, detail=f"Response validation error: {str(e)}")

# Health check endpoint
@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)