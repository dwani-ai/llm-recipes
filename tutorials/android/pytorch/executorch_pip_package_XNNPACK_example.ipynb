{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9au7X0dij6W"
   },
   "source": [
    "This notebook uses the prebuilt `executorch` pip package to export and run an XNNPACK-delegated model. This shows that it's possible to do so without needing to clone the repo and install from source, simplifying the steps currently at https://pytorch.org/executorch/stable/getting-started-setup.html.\n",
    "\n",
    "Note that this will only work if the model is compatible with the operators and backends linked into the `executorch>=0.2.0` package: i.e., it uses the core ATen operator set, and may use the XNNPACK backend. But this example doesn't support custom operators or other backends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qkeAZV3i56Q"
   },
   "source": [
    "Install the prebuilt executorch pip package.\n",
    "\n",
    "NOTE: You may see the message ERROR: pip's dependency resolver ... for packages like torchaudio and torchtext, but it won't affect this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljkj2Fu6i8Pq",
    "outputId": "7b06e68a-7fcc-4b82-d85e-235f7911aca5"
   },
   "outputs": [],
   "source": [
    "!pip install executorch==0.2.1\n",
    "\n",
    "# Instead of 0.2.1, use the latest version in https://pypi.org/project/executorch/#history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqUt0VmWj8X3"
   },
   "source": [
    "Demonstrate that the packages are imported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sMtwCEDEj9JR",
    "outputId": "acb188ae-cdd1-48f7-d5b4-821b877223a4"
   },
   "outputs": [],
   "source": [
    "from executorch import version\n",
    "\n",
    "version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LElQ2xGUkKke",
    "outputId": "b030c8d2-bd9f-4276-9e9a-a0365ca9434c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "# executorch pip package uses torch as dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZHRT7HIkdA-"
   },
   "source": [
    "Demonstrate that the native pybindings module imports successfully, and provides basic operators.\n",
    "\n",
    "Pybindings are great for doing testing and prototyping. Ultimately, you'll need to write the executorch runtime integration in C++ during productionization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_0geuUZCkgN_",
    "outputId": "23a773d1-ca08-499c-ca1e-3204dc65cd15"
   },
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings import portable_lib\n",
    "\n",
    "ops = portable_lib._get_operator_names()\n",
    "f\"Found {len(ops)} ops; first is '{ops[0]}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6UWtEaFk0YX"
   },
   "source": [
    "Let's do inference on MobileNetV2.\n",
    "\n",
    "In order to do that,\n",
    "- first we need to find the nn.Module\n",
    "- use `torch.export()` to export the graph\n",
    "- And subsequently, generate `.pte` file\n",
    "- Lastly, do inference via executorch runtime on the `.pte` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK2ZRrWFlDj2",
    "outputId": "5f079ef2-8149-4c6c-b808-da062cc8a2c2"
   },
   "outputs": [],
   "source": [
    "!pip install 'torchvision==0.18.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWY2InbomP7w",
    "outputId": "1d5a4549-b97a-42b4-8d3a-87e2e85a4503"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "\n",
    "mv2 = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT) # This is torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jwg-R_jcmXo4"
   },
   "outputs": [],
   "source": [
    "from executorch.exir import to_edge\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "\n",
    "model = mv2.eval() # turn into evaluation mode\n",
    "\n",
    "example_inputs = (torch.randn((1, 3, 224, 224)),) # Necessary for exporting the model\n",
    "\n",
    "exported_graph = torch.export.export(model, example_inputs) # Core Aten graph\n",
    "\n",
    "edge = to_edge(exported_graph) # Edge Dialect\n",
    "\n",
    "edge_delegated = edge.to_backend(XnnpackPartitioner()) # Parts of the graph are delegated to XNNPACK\n",
    "\n",
    "executorch_program = edge_delegated.to_executorch() # ExecuTorch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAFKY0tsmwo4"
   },
   "outputs": [],
   "source": [
    "pte_path = \"mv2_xnnpack.pte\"\n",
    "\n",
    "with open(pte_path, \"wb\") as file:\n",
    "    executorch_program.write_to_file(file) # Serializing into .pte file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9jins9_pCxH"
   },
   "source": [
    "Try loading and executing the XNNPACK-delegated model using the executorch pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xR7tZJiYo-oT",
    "outputId": "c54039cb-ae4d-456a-f4d0-58755d91a042"
   },
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings import portable_lib\n",
    "m = portable_lib._load_for_executorch(pte_path)\n",
    "\n",
    "t = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "output = m.forward([t])\n",
    "assert len(output) == 1, f\"Unexpected output length {len(output)}\"\n",
    "assert output[0].size() == torch.Size([1, 1000]), f\"Unexpected output size {output[0].size()}\"\n",
    "print(\"PASS\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
