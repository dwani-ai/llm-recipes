Inference Upgrade 


- Chunking Strategy : context length 

Implement chunking logic in generation.
10 secs audio max / 
Small text size .

Implement for both batch generation and single generation. 

Reduce cycles per call of image. 

Speed improvement with narrator/ long text .

-- 

Quality improvement 
Test audio with parler-tts-large for quality and speed .


-- 